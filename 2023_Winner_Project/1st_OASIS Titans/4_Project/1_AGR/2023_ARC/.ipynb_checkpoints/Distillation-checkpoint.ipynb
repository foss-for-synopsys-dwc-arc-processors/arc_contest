{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3857cea5",
   "metadata": {},
   "source": [
    "# <font color = purple> 2023 Synopsys ARC AIoT Design Contest - Knowledge Distillation </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fc0ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D, DepthwiseConv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import UpSampling2D\n",
    "from tensorflow.keras.layers import Activation, Softmax, ReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Layer, Flatten, Dense, Dropout, add\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45d405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7388aca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "for gpu in gpus:\n",
    "    print(gpu)\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55f8178",
   "metadata": {},
   "source": [
    "## <font color = navy> Configurations </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5feb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    # Data path\n",
    "    IMAGE_DIR = \"./train_img\"\n",
    "    LABEL_DIR = \"./train_label/label.csv\"\n",
    "    \n",
    "    # Degree unit\n",
    "    DEG_UNIT = 6\n",
    "    \n",
    "    #  Dataloader parameters\n",
    "    SPLIT = 0.2\n",
    "    \n",
    "    # Batch size\n",
    "    BATCH_SIZE = 16\n",
    "    \n",
    "    # Image size\n",
    "    IMG_HEIGHT = 224 #224\n",
    "    IMG_WIDTH = 224 #224\n",
    "    \n",
    "    # Model parameters\n",
    "    NUM_CLASSES = int(360 / DEG_UNIT)\n",
    "    IN_CHANNEL = 1\n",
    "    INPUT_SHAPE = (IMG_HEIGHT, IMG_WIDTH, IN_CHANNEL)\n",
    "    \n",
    "    # Learning parameters\n",
    "    lr = 1e-3\n",
    "    num_epochs = 20\n",
    "    teacher_loss_func = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    student_loss_func = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    distill_loss_func = keras.losses.KLDivergence()\n",
    "    loss_func = \"categorical_crossentropy\"\n",
    "    \n",
    "    # Optimizer parameters\n",
    "    opt = 'adam'\n",
    "    \n",
    "    # Distiller parameters\n",
    "    alpha = 0.1      # weight to student_loss_fn and 1-alpha to distill_loss_func\n",
    "    temperature = 3  # Temperature for softening probability distributions. Larger temperature gives softer distributions.\n",
    "    \n",
    "    \n",
    "    # Model\n",
    "    model_name = 'MobileNetV2'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3df3d4",
   "metadata": {},
   "source": [
    "## <font color = navy> Load and Preprocess Dataset </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc98192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(img_path, lab_path, split=0.2):\n",
    "    img_files = sorted(glob.glob(img_path + \"/*\"), key=os.path.getmtime) # Sort by time\n",
    "    # img_files = glob.glob(img_path + \"/*\") (someting wrong)\n",
    "    \n",
    "    lab_df = pd.read_csv(lab_path)['class'].to_numpy() # Read label from csv\n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    \n",
    "    # Concatenate images & labels\n",
    "    for idx, img_file in enumerate(img_files):\n",
    "        # image\n",
    "        img = Image.open(img_file)\n",
    "        img = img.convert('L') # Convert to gray\n",
    "        img = np.array(img, dtype=int)\n",
    "        img = img / 255.0 # Normalize\n",
    "        \n",
    "        # label\n",
    "        lab = np.array(lab_df[idx], dtype=int)\n",
    "        lab = np.expand_dims(lab, axis=0)\n",
    "        \n",
    "        data_x.append(img)\n",
    "        data_y.append(lab)\n",
    "    \n",
    "    # Train & test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, test_size=split, random_state=42, shuffle=True)\n",
    "    \n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1b9b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "X_train, X_test, y_train, y_test = load_data(Config.IMAGE_DIR, Config.LABEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7aff12",
   "metadata": {},
   "source": [
    "## <font color = navy> Show Data </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4d5614",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "for i in range(15):  \n",
    "    ax = fig.add_subplot(3, 5, i+1)\n",
    "    img = Image.fromarray(np.uint8(X_train[i] * 255)) # *255\n",
    "    img = img.convert('RGBA')\n",
    "    \n",
    "    ax.imshow(img, cmap=plt.get_cmap('gray'))\n",
    "    lab = np.squeeze(y_train[i])\n",
    "    lower = lab * Config.DEG_UNIT\n",
    "    upper = (lab+1) * Config.DEG_UNIT\n",
    "    \n",
    "    # ax.set_title('class: {y}'.format(y=np.squeeze(y_train[i])))\n",
    "    ax.set_title('degree: {y1}~{y2}'.format(y1=lower, y2=upper))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b314ef80",
   "metadata": {},
   "source": [
    "## <font color = navy> Reshape Images </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee61e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape\n",
    "X_train = X_train.reshape(-1, Config.IMG_HEIGHT, Config.IMG_WIDTH, 1)\n",
    "X_test = X_test.reshape(-1, Config.IMG_HEIGHT, Config.IMG_WIDTH, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebc08f2",
   "metadata": {},
   "source": [
    "## <font color = navy> Distiller </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75126394",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super().__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1,\n",
    "        temperature=3,\n",
    "    ):\n",
    "        \"\"\" Configure the distiller.\n",
    "\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student weights\n",
    "            metrics: Keras metrics for evaluation\n",
    "            student_loss_fn: Loss function of difference between student\n",
    "                predictions and ground-truth\n",
    "            distillation_loss_fn: Loss function of difference between soft\n",
    "                student predictions and soft teacher predictions\n",
    "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
    "            temperature: Temperature for softening probability distributions.\n",
    "                Larger temperature gives softer distributions.\n",
    "        \"\"\"\n",
    "        super().compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack data\n",
    "        x, y = data\n",
    "\n",
    "        # Forward pass of teacher\n",
    "        teacher_predictions = self.teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student\n",
    "            student_predictions = self.student(x, training=True)\n",
    "\n",
    "            # Compute losses\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)\n",
    "\n",
    "            # Compute scaled distillation loss from https://arxiv.org/abs/1503.02531\n",
    "            # The magnitudes of the gradients produced by the soft targets scale\n",
    "            # as 1/T^2, multiply them by T^2 when using both hard and soft targets.\n",
    "            distillation_loss = (\n",
    "                self.distillation_loss_fn(\n",
    "                    tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
    "                    tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
    "                )\n",
    "                * self.temperature**2\n",
    "            )\n",
    "\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update the metrics configured in `compile()`.\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update(\n",
    "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "\n",
    "        # Compute predictions\n",
    "        y_prediction = self.student(x, training=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        student_loss = self.student_loss_fn(y, y_prediction)\n",
    "\n",
    "        # Update the metrics.\n",
    "        self.compiled_metrics.update_state(y, y_prediction)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f14787",
   "metadata": {},
   "source": [
    "## <font color = navy> Create Teacher Model </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bbbdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the teacher\n",
    "teacher = Sequential(name='teacher')\n",
    "\n",
    "# layer1\n",
    "teacher.add(Conv2D(32, kernel_size=(5, 5), activation='relu', padding='same', input_shape=(Config.IMG_HEIGHT,Config.IMG_WIDTH,1)))\n",
    "teacher.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# layer2\n",
    "teacher.add(Conv2D(64, (5, 5), padding='same', activation='relu'))\n",
    "teacher.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# layer4\n",
    "teacher.add(Flatten())\n",
    "teacher.add(Dense(512, activation='relu'))\n",
    "\n",
    "# layer5\n",
    "teacher.add(Dense(Config.NUM_CLASSES)) # Without softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af25be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# teacher = keras.Sequential(\n",
    "#     [\n",
    "#         keras.Input(shape=(28, 28, 1)),\n",
    "#         layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "#         layers.LeakyReLU(alpha=0.2),\n",
    "#         layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
    "#         layers.Conv2D(512, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "#         layers.Flatten(),\n",
    "#         layers.Dense(10),\n",
    "#     ],\n",
    "#     name=\"teacher\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b753020",
   "metadata": {},
   "source": [
    "## <font color = navy> Create Student Model </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4339cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNetV2\n",
    "def expansion_block(x, t, filters, block_id):\n",
    "    prefix = 'block_{}_'.format(block_id)\n",
    "    total_filter = t*filters\n",
    "    x = Conv2D(total_filter, 1, padding='same',use_bias=False, name=prefix+'expand')(x)\n",
    "    x = BatchNormalization(name=prefix+'expand_bn')(x)\n",
    "    x = ReLU(6, name=prefix+'expand_relu')(x) # ReLU 6\n",
    "    \n",
    "    return x\n",
    "\n",
    "def depthwise_block(x, stride, block_id):\n",
    "    prefix = 'block_{}_'.format(block_id)\n",
    "    x = DepthwiseConv2D(3, strides=(stride,stride), padding='same', use_bias=False, name=prefix +'depthwise_conv')(x)\n",
    "    x = BatchNormalization(name=prefix+'dw_bn')(x)\n",
    "    x = ReLU(6,name=prefix+'dw_relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def projection_block(x, out_channels, block_id):\n",
    "    prefix = 'block_{}_'.format(block_id)\n",
    "    x = Conv2D(filters=out_channels, kernel_size=1, padding='same', use_bias=False, name=prefix+'compress')(x)\n",
    "    x = BatchNormalization(name=prefix+'compress_bn')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f763254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bottleneck(x, t, filters, out_channels, stride, block_id):\n",
    "    y = expansion_block(x, t, filters, block_id)\n",
    "    y = depthwise_block(y, stride, block_id)\n",
    "    y = projection_block(y, out_channels, block_id)\n",
    "    \n",
    "    if y.shape[-1]==x.shape[-1]:\n",
    "        y = add([x,y])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbf3980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MobileNetV2(input_image=Config.INPUT_SHAPE, n_classes=Config.NUM_CLASSES):\n",
    "    input = Input(input_image)\n",
    "\n",
    "    x = Conv2D(32,kernel_size=3,strides=(2,2),padding = 'same', use_bias=False)(input)\n",
    "    x = BatchNormalization(name='conv1_bn')(x)\n",
    "    x = ReLU(6, name = 'conv1_relu')(x)\n",
    "\n",
    "    # 17 Bottlenecks\n",
    "\n",
    "    x = depthwise_block(x,stride=1,block_id=1)\n",
    "    x = projection_block(x, out_channels=16,block_id=1)\n",
    "\n",
    "    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 24, stride = 2,block_id = 2)\n",
    "    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 24, stride = 1,block_id = 3)\n",
    "\n",
    "    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 32, stride = 2,block_id = 4)\n",
    "    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 32, stride = 1,block_id = 5)\n",
    "    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 32, stride = 1,block_id = 6)\n",
    "\n",
    "    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 64, stride = 2,block_id = 7)\n",
    "    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 64, stride = 1,block_id = 8)\n",
    "    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 64, stride = 1,block_id = 9)\n",
    "    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 64, stride = 1,block_id = 10)\n",
    "\n",
    "    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 96, stride = 1,block_id = 11)\n",
    "    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 96, stride = 1,block_id = 12)\n",
    "    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 96, stride = 1,block_id = 13)\n",
    "\n",
    "    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 160, stride = 2,block_id = 14)\n",
    "    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 160, stride = 1,block_id = 15)\n",
    "    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 160, stride = 1,block_id = 16)\n",
    "\n",
    "    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 320, stride = 1,block_id = 17)\n",
    "\n",
    "\n",
    "    #1*1 conv\n",
    "    x = Conv2D(filters = 1280,kernel_size = 1,padding='same',use_bias=False, name = 'last_conv')(x)\n",
    "    x = BatchNormalization(name='last_bn')(x)\n",
    "    x = ReLU(6,name='last_relu')(x)\n",
    "\n",
    "    #AvgPool 7*7\n",
    "    x = GlobalAveragePooling2D(name='global_average_pool')(x)\n",
    "\n",
    "    output = Dense(n_classes)(x) # Without softmax\n",
    "\n",
    "    model = Model(input, output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594bfeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the student\n",
    "student = MobileNetV2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc3cabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# student = keras.Sequential(\n",
    "#     [\n",
    "#         keras.Input(shape=(28, 28, 1)),\n",
    "#         layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "#         layers.LeakyReLU(alpha=0.2),\n",
    "#         layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
    "#         layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "#         layers.Flatten(),\n",
    "#         layers.Dense(10),\n",
    "#     ],\n",
    "#     name=\"student\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e289738c",
   "metadata": {},
   "source": [
    "## <font color = navy> Clone Student Model for Comparison </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone student for later comparison\n",
    "student_scratch = keras.models.clone_model(student)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cda0214",
   "metadata": {},
   "source": [
    "## <font color = navy> Train Teacher Model </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e804532",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher.compile(\n",
    "    optimizer=Config.opt,\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891e410c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate teacher on data.\n",
    "teacher_history = teacher.fit(X_train, y_train,\n",
    "                              validation_split=0.2,\n",
    "                              batch_size=Config.BATCH_SIZE,\n",
    "                              epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a73073",
   "metadata": {},
   "source": [
    "## <font color = navy> Distill Teacher to Student </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cf2578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and compile distiller\n",
    "distiller = Distiller(student=student, teacher=teacher)\n",
    "distiller.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "    alpha=0.1,\n",
    "    temperature=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcf6618",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Distill teacher to student\n",
    "distiller.fit(X_train, y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9107fd08",
   "metadata": {},
   "source": [
    "## <font color = navy> Train </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61764532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train student as doen usually\n",
    "student_scratch.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f664db0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate student trained from scratch.\n",
    "student_history = student_scratch.fit(X_train, y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcc2f1a",
   "metadata": {},
   "source": [
    "## <font color = navy> Evaluation </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5918f5ea",
   "metadata": {},
   "source": [
    "### <font color = red> Teacher Model </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1227cd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_score = teacher.evaluate(X_test, y_test, verbose=0)\n",
    "print(teacher_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e393bff",
   "metadata": {},
   "source": [
    "### <font color = red> Distiller Student Model </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2743df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate student on test dataset\n",
    "distiller_score = distiller.evaluate(X_test, y_test, verbose=0)\n",
    "print(distiller_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd2acea",
   "metadata": {},
   "source": [
    "### <font color = red> Original Student Model </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d25d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_score = student_scratch.evaluate(X_test, y_test, verbose=0)\n",
    "print(student_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ddaf8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
